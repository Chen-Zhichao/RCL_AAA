{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9074b6bd-b056-4d1e-b7ed-ae12c687ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.distributed\n",
    "from torch.utils.data import DataLoader\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from core.configs import cfg\n",
    "from core.datasets import build_dataset\n",
    "from core.models import build_feature_extractor, build_classifier\n",
    "from core.solver import adjust_learning_rate\n",
    "from core.utils.misc import mkdir\n",
    "from core.utils.logger import setup_logger\n",
    "from core.utils.metric_logger import MetricLogger\n",
    "from core.active.build import PixelSelection, RegionSelection\n",
    "from core.datasets.dataset_path_catalog import DatasetCatalog\n",
    "from core.loss.negative_learning_loss import NegativeLearningLoss\n",
    "from core.loss.local_consistent_loss import LocalConsistentLoss\n",
    "from core.utils.utils import set_random_seed\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe35966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 13 03:10:16 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:C3:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    60W / 250W |    471MiB / 40960MiB |     99%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     94052      C   ./cobg_kokkos                     469MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd2a95b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bperlmutter/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accb10f9-b1b1-41db-ab82-215d0f7c7689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-05 20:25:40,712 RCL-AAA INFO: Namespace(config_file='configs/gtav/deeplabv3plus_r101_RA.yaml', opts=['OUTPUT_DIR', 'results/v3plus_gtav_ra_5.0_precent', 'DEBUG', '0'], proctitle='RCL-AAA')\n",
      "2022-10-05 20:25:40,725 RCL-AAA INFO: Loaded configuration file configs/gtav/deeplabv3plus_r101_RA.yaml\n",
      "2022-10-05 20:25:40,726 RCL-AAA INFO: Running with config:\n",
      "ACTIVE:\n",
      "  NAME: RCL-AAA\n",
      "  PIXELS: 40\n",
      "  RADIUS_K: 1\n",
      "  RATIO: 0.05\n",
      "  SELECT_ITER: [10000, 12000, 14000, 16000, 18000]\n",
      "  SETTING: RA\n",
      "DATASETS:\n",
      "  SOURCE_TRAIN: gtav_train\n",
      "  TARGET_TRAIN: cityscapes_train\n",
      "  TEST: cityscapes_val\n",
      "DEBUG: 0\n",
      "INPUT:\n",
      "  IGNORE_LABEL: 255\n",
      "  INPUT_SCALES_TRAIN: (1.0, 1.0)\n",
      "  INPUT_SIZE_TEST: (1280, 640)\n",
      "  PIXEL_MEAN: [0.485, 0.456, 0.406]\n",
      "  PIXEL_STD: [0.229, 0.224, 0.225]\n",
      "  SOURCE_INPUT_SIZE_TRAIN: (1280, 720)\n",
      "  TARGET_INPUT_SIZE_TRAIN: (1280, 640)\n",
      "  TO_BGR255: False\n",
      "MODEL:\n",
      "  DEVICE: cuda\n",
      "  FREEZE_BN: True\n",
      "  NAME: deeplabv3plus_resnet101\n",
      "  NUM_CLASSES: 19\n",
      "  WEIGHTS: https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n",
      "OUTPUT_DIR: results/v3plus_gtav_ra_5.0_precent\n",
      "PREPARE_DIR: \n",
      "SEED: -1\n",
      "SOLVER:\n",
      "  BASE_LR: 0.00025\n",
      "  BATCH_SIZE: 2\n",
      "  BATCH_SIZE_VAL: 1\n",
      "  CHECKPOINT_PERIOD: 1000\n",
      "  CONSISTENT_LOSS: 0.1\n",
      "  LCR_TYPE: l1\n",
      "  LR_METHOD: poly\n",
      "  LR_POWER: 0.9\n",
      "  MAX_ITER: 62500\n",
      "  MOMENTUM: 0.9\n",
      "  NEGATIVE_LOSS: 1.0\n",
      "  NEGATIVE_THRESHOLD: 0.05\n",
      "  STOP_ITER: 62500\n",
      "  WARMUP_ITER: 10000\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "STYLE_DATASETS:\n",
      "  IS_CLASS_STYLE_COMPUTED: False\n",
      "  IS_OVERALL_STYLE_COMPUTED: False\n",
      "TEST:\n",
      "  BATCH_SIZE: 1\n",
      "WARMUP_OUTPUT_DIR: \n",
      "resume: \n",
      "2022-10-05 20:25:40,727 RCL-AAA INFO: Initializing Cityscapes label mask...\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Active Domain Adaptive Semantic Segmentation Training\")\n",
    "parser.add_argument(\"-cfg\",\n",
    "                    \"--config-file\",\n",
    "                    default=\"\",\n",
    "                    metavar=\"FILE\",\n",
    "                    help=\"path to config file\",\n",
    "                    type=str)\n",
    "parser.add_argument(\"--proctitle\",\n",
    "                    type=str,\n",
    "                    default=\"RCL-AAA\",\n",
    "                    help=\"allow a process to change its title\",)\n",
    "parser.add_argument(\n",
    "    \"opts\",\n",
    "    help=\"Modify config options using the command-line\",\n",
    "    default=None,\n",
    "    nargs=argparse.REMAINDER\n",
    ")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = parser.parse_args(args=['-cfg', 'configs/gtav/deeplabv3plus_r101_RA.yaml','--proctitle', 'RCL-AAA'])\n",
    "args.opts = ['OUTPUT_DIR', 'results/v3plus_gtav_ra_5.0_precent', 'DEBUG', '0']\n",
    "\n",
    "if args.opts is not None:\n",
    "    args.opts[-1] = args.opts[-1].strip('\\r\\n')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "cfg.merge_from_file(args.config_file)\n",
    "cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()\n",
    "\n",
    "output_dir = cfg.OUTPUT_DIR\n",
    "if output_dir:\n",
    "    mkdir(output_dir)\n",
    "\n",
    "logger = setup_logger(\"RCL-AAA\", output_dir, 0)\n",
    "logger.info(args)\n",
    "\n",
    "logger.info(\"Loaded configuration file {}\".format(args.config_file))\n",
    "logger.info(\"Running with config:\\n{}\".format(cfg))\n",
    "\n",
    "logger.info('Initializing Cityscapes label mask...')\n",
    "\n",
    "set_random_seed(cfg.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6395075-bd2c-4cdf-9c11-c3a89005eb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is 64 CPU, 0 GPU.\n",
      "load checkpoint from http path: https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcori.nersc.gov/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m feature_extractor \u001b[39m=\u001b[39m build_feature_extractor(cfg)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcori.nersc.gov/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#feature_extractor = nn.DataParallel(feature_extractor)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcori.nersc.gov/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m feature_extractor\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcori.nersc.gov/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m classifier \u001b[39m=\u001b[39m build_classifier(cfg)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcori.nersc.gov/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#classifier = nn.DataParallel(classifier)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/MY_PROJ/lib/python3.8/site-packages/torch/nn/modules/module.py:612\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    610\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 612\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.conda/envs/MY_PROJ/lib/python3.8/site-packages/torch/nn/modules/module.py:359\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    358\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 359\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    361\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    362\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    363\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    364\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    370\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/MY_PROJ/lib/python3.8/site-packages/torch/nn/modules/module.py:359\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    358\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 359\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    361\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    362\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    363\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    364\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    370\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/MY_PROJ/lib/python3.8/site-packages/torch/nn/modules/module.py:381\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m param \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    378\u001b[0m     \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 381\u001b[0m         param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    382\u001b[0m     should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    383\u001b[0m     \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.conda/envs/MY_PROJ/lib/python3.8/site-packages/torch/nn/modules/module.py:610\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    609\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 610\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m~/.conda/envs/MY_PROJ/lib/python3.8/site-packages/torch/cuda/__init__.py:172\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    169\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    173\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    176\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "print(\"Here is {} CPU, {} GPU.\".format(multiprocessing.cpu_count(), torch.cuda.device_count()))\n",
    "logger = logging.getLogger(\"RCL-AAA.trainer\")\n",
    "# tb_writer = SummaryWriter('./{}_{}_tensorboard_log_{}_confidence'.format(cfg.DATASETS.SOURCE_TRAIN.split('_')[0], cfg.DATASETS.TARGET_TRAIN.split('_')[0], cfg.CONFIDENCE.WEIGHT))\n",
    "# print('Tensorboard writer log has been created at {}'.format('./{}_{}_tensorboard_log_{}_confidence'.format(cfg.DATASETS.SOURCE_TRAIN.split('_')[0], cfg.DATASETS.TARGET_TRAIN.split('_')[0], cfg.CONFIDENCE.WEIGHT)))\n",
    "\n",
    "# create network\n",
    "device = torch.device(cfg.MODEL.DEVICE)\n",
    "feature_extractor = build_feature_extractor(cfg)\n",
    "#feature_extractor = nn.DataParallel(feature_extractor)\n",
    "feature_extractor.to(device)\n",
    "\n",
    "classifier = build_classifier(cfg)\n",
    "#classifier = nn.DataParallel(classifier)\n",
    "classifier.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6d5b0-5545-4a75-bdcb-05c5f7121d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init optimizer\n",
    "optimizer_fea = torch.optim.SGD(feature_extractor.parameters(), lr=cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM,\n",
    "                                weight_decay=cfg.SOLVER.WEIGHT_DECAY)\n",
    "optimizer_fea.zero_grad()\n",
    "\n",
    "optimizer_cls = torch.optim.SGD(classifier.parameters(), lr=cfg.SOLVER.BASE_LR * 10, momentum=cfg.SOLVER.MOMENTUM,\n",
    "                                weight_decay=cfg.SOLVER.WEIGHT_DECAY)\n",
    "optimizer_cls.zero_grad()\n",
    "\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7f6d0-d1f0-4439-bc19-848c68e44f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load checkpoint\n",
    "# if cfg.resume:\n",
    "#     logger.info(\"Loading checkpoint from {}\".format(cfg.resume))\n",
    "#     checkpoint = torch.load(cfg.OUTPUT_DIR + '/' + cfg.resume, map_location=torch.device('cpu'))\n",
    "#     iteration = checkpoint['iteration']\n",
    "#     feature_extractor.load_state_dict(checkpoint['feature_extractor'])\n",
    "#     optimizer_fea.load_state_dict(checkpoint['optimizer_fea'])\n",
    "#     classifier.load_state_dict(checkpoint['classifier'])\n",
    "#     optimizer_cls.load_state_dict(checkpoint['optimizer_cls'])\n",
    "# # feature_extractor = nn.DataParallel(feature_extractor)      # modified by CZC\n",
    "# # classifier = nn.DataParallel(classifier)            # modified by CZC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe82208-e27c-421a-8a3d-6a1e8f807bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init mask for cityscape\n",
    "# DatasetCatalog.initMask(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb5a7b-7ba5-4615-b773-99b91222499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train_data = build_dataset(cfg, mode='train', is_source=True)\n",
    "tgt_train_data = build_dataset(cfg, mode='train', is_source=False)\n",
    "tgt_epoch_data = build_dataset(cfg, mode='active', is_source=False, epochwise=True)\n",
    "\n",
    "src_train_loader = DataLoader(src_train_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "                              pin_memory=False, drop_last=True)\n",
    "tgt_train_loader = DataLoader(tgt_train_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "                              pin_memory=False, drop_last=True)\n",
    "tgt_epoch_loader = DataLoader(tgt_epoch_data, batch_size=1, shuffle=False, num_workers=4,\n",
    "                              pin_memory=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69854a4e-da6e-4e20-9c82-215edaa4a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=np.inf)    # added by czc\n",
    "# src_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b0692-42c3-407b-bbd4-5bee2e0a889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src_train_data[0]['img'].size())\n",
    "print(src_train_data[0]['label'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508d165-9a9b-44e4-aecb-0597150d5786",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input, src_label = src_train_data[0]['img'], src_train_data[0]['label']\n",
    "src_input = src_input.cuda(non_blocking=True)\n",
    "src_label = src_label.cuda(non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2076cf-1475-47c3-b69d-9f7a6605d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init loss\n",
    "sup_criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "# negative_criterion = NegativeLearningLoss(threshold=cfg.SOLVER.NEGATIVE_THRESHOLD)\n",
    "# local_consistent_loss = LocalConsistentLoss(cfg.MODEL.NUM_CLASSES, cfg.SOLVER.LCR_TYPE).cuda()\n",
    "\n",
    "\n",
    "start_warmup_time = time.time()\n",
    "end = time.time()\n",
    "max_iters = cfg.SOLVER.MAX_ITER\n",
    "warmup_iters = 10000\n",
    "meters = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "logger.info(\">>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>\")\n",
    "feature_extractor.train()\n",
    "classifier.train()\n",
    "active_round = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f8ca3a-2177-4a1d-b37c-dcd9d1d1954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_size = src_input.shape[-2:]\n",
    "print(src_size)\n",
    "src_input = src_input.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809eef39-89cc-4747-b4e5-a6ac0e602349",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5163eee7-616b-4fd3-93f3-d183d0a6a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_out = classifier(feature_extractor(src_input), size=src_size)\n",
    "print(src_out.size())\n",
    "print(src_out[0,:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2a0a9-728b-4118-9e1d-3b0eebf2684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = torch.softmax(src_out, dim=1)\n",
    "print(predict[0,:,100,1000])\n",
    "print(predict.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18ea31-c83c-4406-943e-e7c63052ba6b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# src_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16922c40-3780-4bcd-ac13-6b88d60ca06e",
   "metadata": {},
   "source": [
    "## Warm Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf778f4a-c209-4f78-a5cf-e9949e848aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=0\n",
    "for batch_index, (src_data, tgt_data) in enumerate(zip(src_train_loader, tgt_train_loader)):       \n",
    "    data_time = time.time() - end\n",
    "\n",
    "    current_lr = adjust_learning_rate(cfg.SOLVER.LR_METHOD, cfg.SOLVER.BASE_LR, iteration, max_iters,\n",
    "                                      power=cfg.SOLVER.LR_POWER)\n",
    "    # tb_writer.add_scalar(tag=\"lr\", scalar_value=current_lr, global_step=iteration)      # added by czc\n",
    "    for index in range(len(optimizer_fea.param_groups)):\n",
    "        optimizer_fea.param_groups[index]['lr'] = current_lr\n",
    "    for index in range(len(optimizer_cls.param_groups)):\n",
    "        optimizer_cls.param_groups[index]['lr'] = current_lr * 10\n",
    "\n",
    "    optimizer_fea.zero_grad()\n",
    "    optimizer_cls.zero_grad()\n",
    "\n",
    "    src_input, src_label = src_data['img'], src_data['label']\n",
    "    src_input = src_input.cuda(non_blocking=True)\n",
    "    src_label = src_label.cuda(non_blocking=True)\n",
    "    print(src_input.size())\n",
    "    print(src_label.size())\n",
    "    print(src_label.size()[0])\n",
    "\n",
    "\n",
    "    src_size = src_input.shape[-2:]\n",
    "    print(src_size)\n",
    "    src_out = classifier(feature_extractor(src_input), size=src_size)\n",
    "    # print(src_out)\n",
    "\n",
    "    predict = torch.softmax(src_out, dim=1)\n",
    "    print(predict.size())\n",
    "\n",
    "    iteration += 1\n",
    "    if iteration == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d6d9e9-01c1-40f1-8c1f-a05dddda4340",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "h = 720\n",
    "w = 1280\n",
    "numparts_h = 2\n",
    "numparts_w = 4\n",
    "parts_h = int(h / numparts_h)\n",
    "parts_w = int(w / numparts_w)\n",
    "batch_size = 2\n",
    "batch_centroids = {}        \n",
    "# batch_centroids:{\n",
    "#                  'img_idx': {\n",
    "#                               {}_{}': {\n",
    "#                                         classID: {centroid}, classID: {centroid}, classID: {centroid}, ...\n",
    "\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "\n",
    "\n",
    "for k in range(batch_size):\n",
    "    batch_centroids['img_idx_{}'.format(k)] = {}\n",
    "    for i in range(numparts_h):\n",
    "        for j in range(numparts_w):\n",
    "            batch_centroids['img_idx_{}'.format(k)]['{}_{}'.format(i,j)] = {} \n",
    "            \n",
    "            # Get region coordinates \n",
    "            if [i,j] == [range(numparts_h)[-1], range(numparts_w)[-1]]:\n",
    "                region_index = [i*parts_h, h-1, j*parts_w, w-1]\n",
    "            else:\n",
    "                region_index = [i*parts_h, (i+1)*parts_h-1, j*parts_w, (j+1)*parts_w-1]\n",
    "            # batch_centroids['img_idx_{}'.format(k)]['region_{}_{}'.format(i,j)]['region_index'] = region_index\n",
    "            \n",
    "            # Get all class ID in a single region\n",
    "            classID = dict(Counter(src_label[k, region_index[0]:region_index[1], region_index[2]:region_index[3]].cpu().numpy().flatten()))\n",
    "            if classID.__contains__(255): del classID[255]\n",
    "            # batch_centroids['img_idx_{}'.format(k)]['region_{}_{}'.format(i,j)]['region_class']['classID'] = classID\n",
    "            \n",
    "            # Get all predict mean as centroids\n",
    "            centroids = {}\n",
    "            for key in classID:\n",
    "                predict_sum = torch.zeros([1,19], requires_grad=True)\n",
    "                predict_sum = predict_sum.cuda(non_blocking=True)\n",
    "                mask = (src_label[k, region_index[0]:region_index[1], region_index[2]:region_index[3]] == key)\n",
    "                predict_mask = predict[k, :, region_index[0]:region_index[1], region_index[2]:region_index[3]] * mask\n",
    "                centroids[key] = predict_mask.sum(axis=[1,2]) / classID[key]\n",
    "\n",
    "\n",
    "            batch_centroids['img_idx_{}'.format(k)]['{}_{}'.format(i,j)] = centroids\n",
    "            # print(batch_centroids)\n",
    "            # print([i,j])\n",
    "            \n",
    "end = time.perf_counter()\n",
    "print(str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa7bd1-2eaf-4a52-a9cd-adae38cde0aa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(batch_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9435f-634a-42a8-a197-87818bd788fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# intra contrastive loss v1\n",
    "num_classes = 19\n",
    "start = time.perf_counter()\n",
    "\n",
    "for k in batch_centroids:\n",
    "    loss = []\n",
    "    # calculate per clas\n",
    "    for cls in range(num_classes):\n",
    "        pos, neg = {}, {}\n",
    "        # pos: {'region_{}_{}': {tensor([...])} }\n",
    "        # neg: {'region_{}_{}': {cls: tensor([...]), cls: tensor([...]), cls: tensor([...]), ...}}\n",
    "        for region in batch_centroids[k]:  # 此时的region是 0_0\n",
    "            neg[region] = {}\n",
    "            for intra_cls in batch_centroids[k][region]:\n",
    "                neg[region][intra_cls] = batch_centroids[k][region][intra_cls]    # 只有这样copy tensor，才不至于改变batch_centroids本身\n",
    "            if batch_centroids[k][region].__contains__(cls):\n",
    "                pos[region] = batch_centroids[k][region][cls]\n",
    "                del neg[region][cls]\n",
    "        print('class_{}:'.format(cls))\n",
    "        print('pos:{}'.format(pos))\n",
    "        # print('neg:{}'.format(neg))\n",
    "end = time.perf_counter()\n",
    "print(str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80911091-f5b5-462b-b2b5-60a2ba4f577b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# intra contrastive loss v2\n",
    "num_classes = 19\n",
    "positive_weight_increment_step = 0.01\n",
    "negative_weight_increment_step = 0.01\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for k in batch_centroids:\n",
    "    loss = []\n",
    "    # calculate per clas\n",
    "    for cls in range(num_classes):\n",
    "        pos, neg = {}, {}\n",
    "        # pos: {'region_{}_{}': {tensor([...])} }\n",
    "        # neg: {'region_{}_{}': {cls: tensor([...]), cls: tensor([...]), cls: tensor([...]), ...}}\n",
    "        for region in batch_centroids[k]:  # 此时的region是 0_0\n",
    "            neg[region] = {}\n",
    "            for intra_cls in batch_centroids[k][region]:\n",
    "                neg[region][intra_cls] = batch_centroids[k][region][intra_cls]    # 只有这样copy tensor，才不至于改变batch_centroids本身\n",
    "            if batch_centroids[k][region].__contains__(cls):\n",
    "                pos[region] = batch_centroids[k][region][cls]\n",
    "                del neg[region][cls]\n",
    "        pos_pairs = []\n",
    "        for pos_region_1 in pos:\n",
    "            pos_pairs.append(pos[pos_region_1]) # The first positive pair.\n",
    "            for pos_region_2 in pos:\n",
    "                if pos_region_2 != pos_region_1:    # 与其它区域的positive centroids构成带权重的positive pairs\n",
    "                    pos_region_1_index = np.array([int(pos_region_1.split('_')[0]), int(pos_region_1.split('_')[1])])\n",
    "                    pos_region_2_index = np.array([int(pos_region_2.split('_')[0]), int(pos_region_2.split('_')[1])])\n",
    "                    positive_weight = 1 - positive_weight_increment_step * np.linalg.norm(pos_region_1_index - pos_region_2_index, ord=2) # L2 norm\n",
    "                    pos_pairs.append(pos[pos_region_2] * positive_weight)\n",
    "                else:\n",
    "                    continue\n",
    "            print(pos_pairs) # 该class，以region为中心所构建的所有pos_pairs\n",
    "            pos_pairs_cl = torch.stack(pos_pairs, dim=0)    # 拼接所有positive pairs\n",
    "\n",
    "        neg_pairs = []\n",
    "        # for neg_region_1 in neg:\n",
    "            \n",
    "                    \n",
    "end = time.perf_counter()\n",
    "print(str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c6478aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(pos_set, neg_set, temperature):\n",
    "    assert pos_set.size() != 0, \"Positive pairs should not be EMPTY!\"\n",
    "    assert neg_set.size() != 0, \"Negative pairs should not be EMPTY!\"\n",
    "\n",
    "    pos_head = torch.index_select(pos_set, 0, torch.tensor([0]))\n",
    "    pos_pairs = torch.mm(pos_head, pos_set.permute(1,0))\n",
    "    neg_pairs = torch.mm(pos_head, neg_set.permute(1,0))\n",
    "\n",
    "    all_pairs = torch.cat([neg_pairs.repeat(pos_pairs.size()[1],1), pos_pairs.permute(1,0)], dim=1)\n",
    "    all_pairs = torch.exp(all_pairs / temperature)\n",
    "\n",
    "    exp_aggregation_row = all_pairs.sum(dim=1, keepdim=True)\n",
    "    frac_row = torch.index_select(all_pairs, 1, torch.tensor([all_pairs.size()[1] - 1])) / exp_aggregation_row\n",
    "    log_row = torch.log(frac_row)\n",
    "    \n",
    "    if pos_set.size()[0] == 1:\n",
    "        cl_loss = log_row * (-1)\n",
    "    else:\n",
    "        cl_loss = torch.mean(log_row[1:,:]) * (-1)\n",
    "    \n",
    "    return cl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8bd6d05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(3.5584)]\n",
      "[tensor(3.5584), tensor(3.5584)]\n",
      "[tensor(3.5584), tensor(3.5584), tensor(3.5584)]\n",
      "[tensor(3.5584), tensor(3.5584), tensor(3.5584), tensor(3.5584)]\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "for i in range(1,5):\n",
    "    cat = torch.tensor([[0.1,0.3,0.6],[0.4,0.5,0.1]])\n",
    "    dog = torch.tensor([[0.7,0.1,0.2],[0.3,0.3,0.4],[0.2,0.3,0.5]])\n",
    "    loss.append(contrastive_loss(cat, dog, 0.05))\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a96e16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([1., 2., 3., 4., 5., 6.])\n",
      "b: tensor([[ 9, 10, 11]])\n",
      "c: tensor([[1., 2., 3., 4., 5., 6.],\n",
      "        [1., 2., 3., 4., 5., 6.],\n",
      "        [1., 2., 3., 4., 5., 6.]])\n",
      "b.T: tensor([[ 9],\n",
      "        [10],\n",
      "        [11]])\n",
      "d: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  9.],\n",
      "        [ 1.,  2.,  3.,  4.,  5.,  6., 10.],\n",
      "        [ 1.,  2.,  3.,  4.,  5.,  6., 11.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 3.0000, 4.0000, 5.0000, 6.0000],\n",
       "        [0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000],\n",
       "        [0.3333, 0.6667, 1.0000, 1.3333, 1.6667, 2.0000]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1,2,3,4,5,6], dtype=torch.float)\n",
    "print(\"a:\", a)\n",
    "b = torch.tensor([9,10,11]).unsqueeze(dim=0)\n",
    "print(\"b:\", b)\n",
    "c = a.repeat(b.size()[1],1)\n",
    "print(\"c:\", c)\n",
    "print(\"b.T:\", b.permute(1,0))\n",
    "d = torch.cat((c,b.permute(1,0)), dim=1)\n",
    "print(\"d:\", d)\n",
    "a/torch.tensor([[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af36f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intra contrastive loss v3\n",
    "num_classes = 19\n",
    "positive_weight_increment_step = 0.01\n",
    "negative_weight_increment_step = 0.01\n",
    "temperature = 0.07\n",
    "\n",
    "start = time.perf_counter()\n",
    "loss = []\n",
    "\n",
    "for k in batch_centroids:\n",
    "    # calculate per clas\n",
    "    for cls in range(num_classes):\n",
    "        pos, neg = {}, {}\n",
    "        # pos: {'region_{}_{}': {tensor([...])} }\n",
    "        # neg: {'region_{}_{}': {cls: tensor([...]), cls: tensor([...]), cls: tensor([...]), ...}}\n",
    "        for region in batch_centroids[k]:  # 此时的region是 0_0\n",
    "            neg[region] = {}\n",
    "            for intra_cls in batch_centroids[k][region]:\n",
    "                neg[region][intra_cls] = batch_centroids[k][region][intra_cls]    # 只有这样copy tensor，才不至于改变batch_centroids本身\n",
    "            if batch_centroids[k][region].__contains__(cls):\n",
    "                pos[region] = batch_centroids[k][region][cls]\n",
    "                del neg[region][cls]\n",
    "\n",
    "        pos_region = pos.keys() # positive pairs所在的region\n",
    "        neg_region = neg.keys() # negative pairs所在的region\n",
    "        all_region = list(set(pos_region + neg_region)) # 合并两个list，并且删除重复元素\n",
    "        for region_1 in all_region:\n",
    "            pos_per_region, neg_per_region = [], []         # 该类以该region为中心的positive和negative pairs\n",
    "            cl_per_region = []\n",
    "            region_1_index = np.array([int(region_1.split('_')[0]), int(region_1.split('_')[1])])\n",
    "            if pos.__contains__(region_1):\n",
    "                pos_per_region.append(pos[region_1])        # positive pairs的头，即cls在region_1的centroids\n",
    "            for neg_cls_1 in neg[region_1]:\n",
    "                neg_per_region.append(neg[region_1][neg_cls_1])   # negatiave pairs的头，即在region_1中除了cls之外的所有centroids\n",
    "            for region_2 in all_region:                         # 收集其它region的positive pairs和negative pairs\n",
    "                if region_2 != region_1:\n",
    "                    region_2_index = np.array([int(region_2.split('_')[0]), int(region_2.split('_')[1])])\n",
    "                    positive_weight = 1 + positive_weight_increment_step * np.linalg.norm(region_1_index - region_2_index, ord=2) # L2 norm\n",
    "                    negative_weight = 1 - negative_weight_increment_step * np.linalg.norm(region_1_index - region_2_index, ord=2) # L2 norm\n",
    "                    if pos.__contains__(region_2):     # 其它区域的positive pairs，将会乘上权重\n",
    "                        pos_per_region.append(pos[region_2] * positive_weight)\n",
    "                    if neg.__contains__(region_2):     # 其它区域的negative pairs，将会乘上权重\n",
    "                        for neg_cls_2 in neg[region_2]:\n",
    "                            neg_per_region.append(neg[region_2][neg_cls_2] * negative_weight)\n",
    "            pos_set_cl = torch.stack(pos_per_region, dim=0)       # 第一行tensor就是用于query的positive pair头，剩下都是所有乘上权重后的positive pairs\n",
    "            neg_set_cl = torch.stack(neg_per_region, dim=0)       # 所有乘上权重后的negative pairs\n",
    "            \n",
    "            cl_per_region = contrastive_loss(pos_set_cl, neg_set_cl, temperature)\n",
    "\n",
    "            loss.append([cl_per_region])\n",
    "\n",
    "\n",
    "        \n",
    "        # for neg_region_1 in neg:\n",
    "            \n",
    "print(loss)              \n",
    "end = time.perf_counter()\n",
    "print(str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07120b-e47e-4177-97af-c8b429b64efa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[3 5]\n",
      "[-3 -4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_region_1 = '0_1'\n",
    "pos_region_2 = '3_5'\n",
    "pos_region_1_index = np.array([int(pos_region_1.split('_')[0]), int(pos_region_1.split('_')[1])])\n",
    "pos_region_2_index = np.array([int(pos_region_2.split('_')[0]), int(pos_region_2.split('_')[1])])\n",
    "print(pos_region_1_index)\n",
    "print(pos_region_2_index)\n",
    "print(pos_region_1_index - pos_region_2_index)\n",
    "np.linalg.norm(pos_region_1_index - pos_region_2_index, ord=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('MY_PROJ')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "02f5b6f99c7d7520503bf7e72d414878432f2f5acd2aefdb81eb5ace38f206e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
