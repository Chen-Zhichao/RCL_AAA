{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9074b6bd-b056-4d1e-b7ed-ae12c687ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.distributed\n",
    "from torch.utils.data import DataLoader\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from core.configs import cfg\n",
    "from core.datasets import build_dataset\n",
    "from core.models import build_feature_extractor, build_classifier\n",
    "from core.solver import adjust_learning_rate\n",
    "from core.utils.misc import mkdir\n",
    "from core.utils.logger import setup_logger\n",
    "from core.utils.metric_logger import MetricLogger\n",
    "from core.active.build import PixelSelection, RegionSelection\n",
    "from core.datasets.dataset_path_catalog import DatasetCatalog\n",
    "from core.loss.negative_learning_loss import NegativeLearningLoss\n",
    "from core.loss.local_consistent_loss import LocalConsistentLoss\n",
    "from core.utils.utils import set_random_seed\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe35966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 18 02:09:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:C3:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    33W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd2a95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accb10f9-b1b1-41db-ab82-215d0f7c7689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-18 02:09:32,376 RCL-AAA INFO: Namespace(config_file='configs/gtav/deeplabv3plus_r101_RA.yaml', opts=['OUTPUT_DIR', 'results/v3plus_gtav_ra_5.0_precent', 'DEBUG', '0'], proctitle='RCL-AAA')\n",
      "2022-10-18 02:09:32,393 RCL-AAA INFO: Loaded configuration file configs/gtav/deeplabv3plus_r101_RA.yaml\n",
      "2022-10-18 02:09:32,394 RCL-AAA INFO: Running with config:\n",
      "ACTIVE:\n",
      "  NAME: RCL-AAA\n",
      "  PIXELS: 40\n",
      "  RADIUS_K: 1\n",
      "  RATIO: 0.05\n",
      "  SELECT_ITER: [10000, 12000, 14000, 16000, 18000]\n",
      "  SETTING: RA\n",
      "DATASETS:\n",
      "  SOURCE_TRAIN: gtav_train\n",
      "  TARGET_TRAIN: cityscapes_train\n",
      "  TEST: cityscapes_val\n",
      "DEBUG: 0\n",
      "INPUT:\n",
      "  IGNORE_LABEL: 255\n",
      "  INPUT_SCALES_TRAIN: (1.0, 1.0)\n",
      "  INPUT_SIZE_TEST: (1280, 640)\n",
      "  PIXEL_MEAN: [0.485, 0.456, 0.406]\n",
      "  PIXEL_STD: [0.229, 0.224, 0.225]\n",
      "  SOURCE_INPUT_SIZE_TRAIN: (1280, 720)\n",
      "  TARGET_INPUT_SIZE_TRAIN: (1280, 640)\n",
      "  TO_BGR255: False\n",
      "LOSS:\n",
      "  NEG_WEIGHT_INCRE_STEP: 0.02\n",
      "  NUMPARTS_H: 2\n",
      "  NUMPARTS_W: 4\n",
      "  POS_WEIGHT_INCRE_STEP: 0.02\n",
      "  TEMPERATURE: 0.05\n",
      "MODEL:\n",
      "  DEVICE: cuda\n",
      "  FREEZE_BN: True\n",
      "  NAME: deeplabv3plus_resnet101\n",
      "  NUM_CLASSES: 19\n",
      "  WEIGHTS: https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n",
      "OUTPUT_DIR: results/v3plus_gtav_ra_5.0_precent\n",
      "PREPARE_DIR: \n",
      "SEED: -1\n",
      "SOLVER:\n",
      "  BASE_LR: 1e-05\n",
      "  BATCH_SIZE: 2\n",
      "  BATCH_SIZE_VAL: 1\n",
      "  CHECKPOINT_PERIOD: 1000\n",
      "  CONSISTENT_LOSS: 0.1\n",
      "  LCR_TYPE: l1\n",
      "  LR_METHOD: poly\n",
      "  LR_POWER: 0.9\n",
      "  MAX_ITER: 1000\n",
      "  MOMENTUM: 0.9\n",
      "  NEGATIVE_LOSS: 1.0\n",
      "  NEGATIVE_THRESHOLD: 0.05\n",
      "  STOP_ITER: 1000\n",
      "  WARMUP_ITER: 10000\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "STYLE_DATASETS:\n",
      "  IS_CLASS_STYLE_COMPUTED: False\n",
      "  IS_OVERALL_STYLE_COMPUTED: False\n",
      "TEST:\n",
      "  BATCH_SIZE: 1\n",
      "WARMUP_OUTPUT_DIR: \n",
      "resume: \n",
      "2022-10-18 02:09:32,395 RCL-AAA INFO: Initializing Cityscapes label mask...\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Active Domain Adaptive Semantic Segmentation Training\")\n",
    "parser.add_argument(\"-cfg\",\n",
    "                    \"--config-file\",\n",
    "                    default=\"\",\n",
    "                    metavar=\"FILE\",\n",
    "                    help=\"path to config file\",\n",
    "                    type=str)\n",
    "parser.add_argument(\"--proctitle\",\n",
    "                    type=str,\n",
    "                    default=\"RCL-AAA\",\n",
    "                    help=\"allow a process to change its title\",)\n",
    "parser.add_argument(\n",
    "    \"opts\",\n",
    "    help=\"Modify config options using the command-line\",\n",
    "    default=None,\n",
    "    nargs=argparse.REMAINDER\n",
    ")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = parser.parse_args(args=['-cfg', 'configs/gtav/deeplabv3plus_r101_RA.yaml','--proctitle', 'RCL-AAA'])\n",
    "args.opts = ['OUTPUT_DIR', 'results/v3plus_gtav_ra_5.0_precent', 'DEBUG', '0']\n",
    "\n",
    "if args.opts is not None:\n",
    "    args.opts[-1] = args.opts[-1].strip('\\r\\n')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "cfg.merge_from_file(args.config_file)\n",
    "cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()\n",
    "\n",
    "output_dir = cfg.OUTPUT_DIR\n",
    "if output_dir:\n",
    "    mkdir(output_dir)\n",
    "\n",
    "logger = setup_logger(\"RCL-AAA\", output_dir, 0)\n",
    "logger.info(args)\n",
    "\n",
    "logger.info(\"Loaded configuration file {}\".format(args.config_file))\n",
    "logger.info(\"Running with config:\\n{}\".format(cfg))\n",
    "\n",
    "logger.info('Initializing Cityscapes label mask...')\n",
    "\n",
    "set_random_seed(cfg.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6395075-bd2c-4cdf-9c11-c3a89005eb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is 256 CPU, 1 GPU.\n",
      "load checkpoint from http path: https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is {} CPU, {} GPU.\".format(multiprocessing.cpu_count(), torch.cuda.device_count()))\n",
    "logger = logging.getLogger(\"RCL-AAA.trainer\")\n",
    "# tb_writer = SummaryWriter('./{}_{}_tensorboard_log_{}_confidence'.format(cfg.DATASETS.SOURCE_TRAIN.split('_')[0], cfg.DATASETS.TARGET_TRAIN.split('_')[0], cfg.CONFIDENCE.WEIGHT))\n",
    "# print('Tensorboard writer log has been created at {}'.format('./{}_{}_tensorboard_log_{}_confidence'.format(cfg.DATASETS.SOURCE_TRAIN.split('_')[0], cfg.DATASETS.TARGET_TRAIN.split('_')[0], cfg.CONFIDENCE.WEIGHT)))\n",
    "\n",
    "# create network\n",
    "device = torch.device(cfg.MODEL.DEVICE)\n",
    "feature_extractor = build_feature_extractor(cfg)\n",
    "#feature_extractor = nn.DataParallel(feature_extractor)\n",
    "feature_extractor.to(device)\n",
    "\n",
    "classifier = build_classifier(cfg)\n",
    "#classifier = nn.DataParallel(classifier)\n",
    "classifier.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d6d5b0-5545-4a75-bdcb-05c5f7121d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init optimizer\n",
    "optimizer_fea = torch.optim.SGD(feature_extractor.parameters(), lr=cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM,\n",
    "                                weight_decay=cfg.SOLVER.WEIGHT_DECAY)\n",
    "optimizer_fea.zero_grad()\n",
    "\n",
    "optimizer_cls = torch.optim.SGD(classifier.parameters(), lr=cfg.SOLVER.BASE_LR * 10, momentum=cfg.SOLVER.MOMENTUM,\n",
    "                                weight_decay=cfg.SOLVER.WEIGHT_DECAY)\n",
    "optimizer_cls.zero_grad()\n",
    "\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f7f6d0-d1f0-4439-bc19-848c68e44f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load checkpoint\n",
    "# if cfg.resume:\n",
    "#     logger.info(\"Loading checkpoint from {}\".format(cfg.resume))\n",
    "#     checkpoint = torch.load(cfg.OUTPUT_DIR + '/' + cfg.resume, map_location=torch.device('cpu'))\n",
    "#     iteration = checkpoint['iteration']\n",
    "#     feature_extractor.load_state_dict(checkpoint['feature_extractor'])\n",
    "#     optimizer_fea.load_state_dict(checkpoint['optimizer_fea'])\n",
    "#     classifier.load_state_dict(checkpoint['classifier'])\n",
    "#     optimizer_cls.load_state_dict(checkpoint['optimizer_cls'])\n",
    "# # feature_extractor = nn.DataParallel(feature_extractor)      # modified by CZC\n",
    "# # classifier = nn.DataParallel(classifier)            # modified by CZC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe82208-e27c-421a-8a3d-6a1e8f807bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init mask for cityscape\n",
    "# DatasetCatalog.initMask(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dfb5a7b-7ba5-4615-b773-99b91222499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    <core.datasets.transform.Resize object at 0x152b34922e80>\n",
      "    <core.datasets.transform.ToTensor object at 0x152b34922d60>\n",
      "    <core.datasets.transform.Normalize object at 0x152b34922d90>\n",
      ")\n",
      "Compose(\n",
      "    <core.datasets.transform.Resize object at 0x152b54234730>\n",
      "    <core.datasets.transform.ToTensor object at 0x152b34922fd0>\n",
      "    <core.datasets.transform.Normalize object at 0x152b34922e20>\n",
      ")\n",
      "Compose(\n",
      "    <core.datasets.transform.Resize object at 0x152b34a216d0>\n",
      "    <core.datasets.transform.ToTensor object at 0x152b54db9880>\n",
      "    <core.datasets.transform.Normalize object at 0x152b54234910>\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "src_train_data = build_dataset(cfg, mode='train', is_source=True)\n",
    "tgt_train_data = build_dataset(cfg, mode='train', is_source=False)\n",
    "tgt_epoch_data = build_dataset(cfg, mode='active', is_source=False, epochwise=True)\n",
    "\n",
    "src_train_loader = DataLoader(src_train_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "                              pin_memory=False, drop_last=True)\n",
    "tgt_train_loader = DataLoader(tgt_train_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "                              pin_memory=False, drop_last=True)\n",
    "tgt_epoch_loader = DataLoader(tgt_epoch_data, batch_size=1, shuffle=False, num_workers=4,\n",
    "                              pin_memory=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69854a4e-da6e-4e20-9c82-215edaa4a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=np.inf)    # added by czc\n",
    "# src_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc0b0692-42c3-407b-bbd4-5bee2e0a889c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([720, 1280])\n"
     ]
    }
   ],
   "source": [
    "print(src_train_data[0]['img'].size())\n",
    "print(src_train_data[0]['label'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1508d165-9a9b-44e4-aecb-0597150d5786",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input, src_label = src_train_data[0]['img'], src_train_data[0]['label']\n",
    "src_input = src_input.cuda(non_blocking=True)\n",
    "src_label = src_label.cuda(non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af2076cf-1475-47c3-b69d-9f7a6605d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-18 02:09:46,348 RCL-AAA.trainer INFO: >>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "# init loss\n",
    "sup_criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "# negative_criterion = NegativeLearningLoss(threshold=cfg.SOLVER.NEGATIVE_THRESHOLD)\n",
    "# local_consistent_loss = LocalConsistentLoss(cfg.MODEL.NUM_CLASSES, cfg.SOLVER.LCR_TYPE).cuda()\n",
    "\n",
    "\n",
    "start_warmup_time = time.time()\n",
    "end = time.time()\n",
    "max_iters = cfg.SOLVER.MAX_ITER\n",
    "warmup_iters = 10000\n",
    "meters = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "logger.info(\">>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>\")\n",
    "feature_extractor.train()\n",
    "classifier.train()\n",
    "active_round = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16922c40-3780-4bcd-ac13-6b88d60ca06e",
   "metadata": {},
   "source": [
    "## Warm Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf778f4a-c209-4f78-a5cf-e9949e848aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=0\n",
    "for batch_index, (src_data, tgt_data) in enumerate(zip(src_train_loader, tgt_train_loader)):       \n",
    "    data_time = time.time() - end\n",
    "\n",
    "    current_lr = adjust_learning_rate(cfg.SOLVER.LR_METHOD, cfg.SOLVER.BASE_LR, iteration, max_iters,\n",
    "                                      power=cfg.SOLVER.LR_POWER)\n",
    "    # tb_writer.add_scalar(tag=\"lr\", scalar_value=current_lr, global_step=iteration)      # added by czc\n",
    "    for index in range(len(optimizer_fea.param_groups)):\n",
    "        optimizer_fea.param_groups[index]['lr'] = current_lr\n",
    "    for index in range(len(optimizer_cls.param_groups)):\n",
    "        optimizer_cls.param_groups[index]['lr'] = current_lr * 10\n",
    "\n",
    "    optimizer_fea.zero_grad()\n",
    "    optimizer_cls.zero_grad()\n",
    "\n",
    "    src_input, src_label = src_data['img'], src_data['label']\n",
    "    src_input = src_input.cuda(non_blocking=True)\n",
    "    src_label = src_label.cuda(non_blocking=True)\n",
    "    # print(src_input.size())\n",
    "    # print(src_label.size())\n",
    "    # print(src_label.size()[0])\n",
    "\n",
    "    tgt_input, tgt_mask = tgt_data['img'], tgt_data['mask']\n",
    "    tgt_input = tgt_input.cuda(non_blocking=True)\n",
    "    tgt_mask = tgt_mask.cuda(non_blocking=True)\n",
    "\n",
    "\n",
    "    src_size = src_input.shape[-2:]\n",
    "    # print(src_size)\n",
    "    src_out = classifier(feature_extractor(src_input), size=src_size)\n",
    "    \n",
    "    tgt_size = tgt_input.shape[-2:]\n",
    "    tgt_out = classifier(feature_extractor(tgt_input), size=tgt_size)\n",
    "    predict = torch.softmax(tgt_out, dim=1)\n",
    "\n",
    "    # target active supervision loss\n",
    "    if torch.sum((tgt_mask != 255)) != 0:  # target has labeled pixels\n",
    "        loss_sup_tgt = sup_criterion(tgt_out, tgt_mask)\n",
    "        # meters.update(loss_sup_tgt=loss_sup_tgt.item())\n",
    "\n",
    "\n",
    "\n",
    "    # predict = torch.softmax(src_out, dim=1)\n",
    "\n",
    "    iteration += 1\n",
    "    if iteration == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5299dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_criterion = nn.CrossEntropyLoss(ignore_index=255, reduction='none')\n",
    "# predict[0,:,0:10,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e62217a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 640, 1280])\n"
     ]
    }
   ],
   "source": [
    "tgt_label = torch.argmax(predict[:,:,:,:],dim=1)\n",
    "print(tgt_label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3d1ed01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6,  6,  6,  6,  6],\n",
      "        [ 6,  6,  6,  6,  6],\n",
      "        [ 6,  6,  6,  6, 14],\n",
      "        [ 6,  6,  6,  6, 14],\n",
      "        [ 6,  6,  6,  6, 14]], device='cuda:0')\n",
      "torch.Size([5, 5])\n",
      "tensor([[2.3433, 2.2887, 2.2379, 2.1907, 2.1471],\n",
      "        [2.3106, 2.2600, 2.2134, 2.1707, 2.1317],\n",
      "        [2.2795, 2.2331, 2.1911, 2.1532, 2.1166],\n",
      "        [2.2500, 2.2079, 2.1707, 2.1382, 2.0971],\n",
      "        [2.2221, 2.1845, 2.1524, 2.1256, 2.0806]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "torch.return_types.topk(\n",
      "values=tensor([[2.1471, 2.1907, 2.2379],\n",
      "        [2.1317, 2.1707, 2.2134],\n",
      "        [2.1166, 2.1532, 2.1911],\n",
      "        [2.0971, 2.1382, 2.1707],\n",
      "        [2.0806, 2.1256, 2.1524]], device='cuda:0', grad_fn=<TopkBackward>),\n",
      "indices=tensor([[4, 3, 2],\n",
      "        [4, 3, 2],\n",
      "        [4, 3, 2],\n",
      "        [4, 3, 2],\n",
      "        [4, 3, 2]], device='cuda:0'))\n",
      "tensor(2.0806, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(2.3433, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "tgt_label = torch.argmax(predict[0,:,0:5,0:5],dim=0)\n",
    "print(tgt_label)\n",
    "print(tgt_label.size())\n",
    "# print(tgt_out[0,0:3,0:2,0:2])\n",
    "loss_sup_tgt = sup_criterion(tgt_out[0,0:19,0:5,0:5].permute(1,0,2), tgt_label)\n",
    "print(loss_sup_tgt)\n",
    "print(torch.topk(loss_sup_tgt, k=3, dim=1, largest=False))\n",
    "print(torch.min(loss_sup_tgt))\n",
    "print(torch.max(loss_sup_tgt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098485f9",
   "metadata": {},
   "source": [
    "### RegionSplit_CentroidCal函数编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65d6d9e9-01c1-40f1-8c1f-a05dddda4340",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: 117940, 14: 16264, 0: 430, 9: 245361, 4: 3991, 8: 70, 13: 11882, 3: 9107, 18: 78, 1: 3355, 17: 482}\n",
      "{6: 147136, 14: 24933, 4: 108, 9: 224833, 3: 3566, 13: 6118, 1: 1308, 18: 269, 17: 101, 0: 588}\n",
      "{3: 57364, 6: 171679, 4: 22884, 13: 4220, 9: 88047, 14: 53254, 1: 7784, 16: 90, 0: 2738, 18: 216, 8: 684}\n",
      "{6: 169796, 14: 79845, 4: 6258, 3: 28963, 13: 11436, 0: 1358, 9: 98483, 1: 10065, 16: 762, 18: 1840, 17: 154}\n",
      "0.3419200790813193\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from cv2 import threshold\n",
    "import math\n",
    "h = 720\n",
    "w = 1280\n",
    "numparts_h = 1\n",
    "numparts_w = 2\n",
    "parts_h = int(h / numparts_h)\n",
    "parts_w = int(w / numparts_w)\n",
    "batch_size = 2\n",
    "batch_centroids = {}        \n",
    "is_source = False\n",
    "tgt_centroids_base_ratio = 0.9999\n",
    "\n",
    "if is_source:\n",
    "    predict = torch.softmax(src_out, dim=1)\n",
    "else:\n",
    "    predict = torch.softmax(tgt_out, dim=1)\n",
    "# batch_centroids:{\n",
    "#                  'img_idx': {\n",
    "#                               {}_{}': {\n",
    "#                                         classID: {centroid}, classID: {centroid}, classID: {centroid}, ...\n",
    "cross_entropy_computation = nn.CrossEntropyLoss(ignore_index=255, reduction='none')\n",
    "\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "\n",
    "tgt_label = torch.argmax(predict, dim=1)\n",
    "\n",
    "\n",
    "for k in range(batch_size):\n",
    "    batch_centroids[k] = {}     # k是batch里面img的序号\n",
    "    for i in range(numparts_h):\n",
    "        for j in range(numparts_w):\n",
    "            batch_centroids[k]['{}_{}'.format(i,j)] = {} \n",
    "            \n",
    "            # Get region coordinates \n",
    "            if [i,j] == [range(numparts_h)[-1], range(numparts_w)[-1]]:\n",
    "                rg_id = [i*parts_h, h-1, j*parts_w, w-1]                 # rg_id: region_index\n",
    "            else:\n",
    "                rg_id = [i*parts_h, (i+1)*parts_h-1, j*parts_w, (j+1)*parts_w-1]\n",
    "            # batch_centroids['img_idx_{}'.format(k)]['region_{}_{}'.format(i,j)]['rg_id'] = rg_id\n",
    "            \n",
    "            # Get all class ID in a single region\n",
    "            if is_source == True:\n",
    "                classID = dict(Counter(src_label[k, rg_id[0]:rg_id[1], rg_id[2]:rg_id[3]].cpu().numpy().flatten()))     # 放到代码文件中得改一下\n",
    "            else:\n",
    "                classID = dict(Counter(tgt_label[k, rg_id[0]:rg_id[1], rg_id[2]:rg_id[3]].cpu().numpy().flatten()))     # 放到代码文件中得改一下\n",
    "            \n",
    "            if classID.__contains__(255): del classID[255]\n",
    "            print(classID)\n",
    "            # batch_centroids['img_idx_{}'.format(k)]['region_{}_{}'.format(i,j)]['region_class']['classID'] = classID\n",
    "            \n",
    "            # Get all predict mean as centroids\n",
    "            centroids = {}\n",
    "            for key in classID:\n",
    "                predict_sum = torch.zeros([1,19], requires_grad=True)\n",
    "                predict_sum = predict_sum.cuda(non_blocking=True)\n",
    "                if is_source == True:\n",
    "                    mask = src_label[k, rg_id[0]:rg_id[1], rg_id[2]:rg_id[3]].eq(key)\n",
    "                else:\n",
    "                    origin_tgt_mask = tgt_label[k, rg_id[0]:rg_id[1], rg_id[2]:rg_id[3]].eq(key)       # 代表是这个类的\n",
    "\n",
    "                    tgt_ce = cross_entropy_computation(tgt_out[k, :, rg_id[0]:rg_id[1], rg_id[2]:rg_id[3]].permute(1,0,2), \\\n",
    "                                                        tgt_label[k, rg_id[0]:rg_id[1], rg_id[2]:rg_id[3]])\n",
    "                    tgt_cls_uncertainty = tgt_ce * origin_tgt_mask      # 取最小的几个值注意可能会取到0\n",
    "                    \n",
    "                    unselected_sample_num = math.ceil(origin_tgt_mask.sum().item() * (1 - tgt_centroids_base_ratio))\n",
    "                    unselected_samples, _ = torch.topk(torch.flatten(tgt_cls_uncertainty), k=unselected_sample_num, dim=-1, largest=True)\n",
    "                    uncertainty_thres = unselected_samples.min().item()\n",
    "                    \n",
    "                    uncertainty_mask = tgt_cls_uncertainty.le(uncertainty_thres)        # 代表uncertainty不会过高的样本\n",
    "\n",
    "                    mask = origin_tgt_mask * uncertainty_mask\n",
    "                    \n",
    "                predict_mask = predict[k, :, rg_id[0]:rg_id[1], rg_id[2]:rg_id[3]] * mask\n",
    "                centroids[key] = predict_mask.sum(axis=[1,2]) / classID[key]\n",
    "\n",
    "\n",
    "            batch_centroids[k]['{}_{}'.format(i,j)] = centroids\n",
    "            \n",
    "end = time.perf_counter()\n",
    "print(str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1740147",
   "metadata": {},
   "source": [
    "### contrastive_loss函数编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6478aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(pos_set, neg_set, temperature):\n",
    "    assert pos_set.size() != 0, \"Positive pairs should not be EMPTY!\"\n",
    "    assert neg_set.size() != 0, \"Negative pairs should not be EMPTY!\"\n",
    "\n",
    "    pos_head = torch.index_select(pos_set, 0, torch.tensor([0]).cuda(non_blocking=True))\n",
    "    pos_pairs = torch.mm(pos_head, pos_set.permute(1,0))\n",
    "    neg_pairs = torch.mm(pos_head, neg_set.permute(1,0))\n",
    "\n",
    "    all_pairs = torch.cat([neg_pairs.repeat(pos_pairs.size()[1],1), pos_pairs.permute(1,0)], dim=1)\n",
    "    all_pairs = torch.exp(all_pairs / temperature)\n",
    "\n",
    "    exp_aggregation_row = all_pairs.sum(dim=1, keepdim=True)\n",
    "    frac_row = torch.index_select(all_pairs, 1, torch.tensor([all_pairs.size()[1] - 1]).cuda(non_blocking=True)) / exp_aggregation_row\n",
    "    log_row = torch.log(frac_row)\n",
    "    \n",
    "    if pos_set.size()[0] == 1:\n",
    "        cl_loss = torch.mean(log_row) * (-1)\n",
    "    else:\n",
    "        cl_loss = torch.mean(log_row[1:,:]) * (-1)\n",
    "    \n",
    "    return cl_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa18677",
   "metadata": {},
   "source": [
    "### intra-image level contrastive loss 编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7af36f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8694, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# intra contrastive loss v3\n",
    "\n",
    "from core.loss.contrastive_loss import ContrastiveLoss\n",
    "\n",
    "num_classes = 19\n",
    "positive_weight_increment_step = 0.01\n",
    "negative_weight_increment_step = 0.01\n",
    "temperature = 0.07\n",
    "\n",
    "loss = []\n",
    "\n",
    "contrastive_loss_criterion = ContrastiveLoss()\n",
    "\n",
    "for k in batch_centroids:\n",
    "    # calculate per clas\n",
    "    for cls in range(num_classes):\n",
    "        pos, neg = {}, {}\n",
    "        # pos: {'region_{}_{}': {tensor([...])} }\n",
    "        # neg: {'region_{}_{}': {cls: tensor([...]), cls: tensor([...]), cls: tensor([...]), ...}}\n",
    "        for region in batch_centroids[k]:  # 此时的region是 0_0\n",
    "            neg[region] = {}\n",
    "            for intra_cls in batch_centroids[k][region]:\n",
    "                neg[region][intra_cls] = batch_centroids[k][region][intra_cls]    # 只有这样copy tensor，才不至于改变batch_centroids本身\n",
    "            if batch_centroids[k][region].__contains__(cls):\n",
    "                pos[region] = batch_centroids[k][region][cls]\n",
    "                del neg[region][cls]\n",
    "        pos_region, neg_region = [], []\n",
    "        pos_region = list(pos.keys()) # positive pairs所在的region\n",
    "        neg_region = list(neg.keys()) # negative pairs所在的region\n",
    "        all_region = list(set(pos_region + neg_region)) # 合并两个list，并且删除重复元素\n",
    "        for region_1 in all_region:\n",
    "            pos_per_region, neg_per_region = [], []         # 该类以该region为中心的positive和negative pairs\n",
    "            cl_per_region = []\n",
    "            region_1_index = np.array([int(region_1.split('_')[0]), int(region_1.split('_')[1])])\n",
    "            if pos.__contains__(region_1):\n",
    "                pos_per_region.append(pos[region_1])        # positive pairs的头，即cls在region_1的centroids\n",
    "            for neg_cls_1 in neg[region_1]:\n",
    "                neg_per_region.append(neg[region_1][neg_cls_1])   # negatiave pairs的头，即在region_1中除了cls之外的所有centroids\n",
    "            for region_2 in all_region:                         # 收集其它region的positive pairs和negative pairs\n",
    "                if region_2 != region_1:\n",
    "                    region_2_index = np.array([int(region_2.split('_')[0]), int(region_2.split('_')[1])])\n",
    "                    positive_weight = 1 + positive_weight_increment_step * np.linalg.norm(region_1_index - region_2_index, ord=2) # L2 norm\n",
    "                    negative_weight = 1 - negative_weight_increment_step * np.linalg.norm(region_1_index - region_2_index, ord=2) # L2 norm\n",
    "                    if pos.__contains__(region_2):     # 其它区域的positive pairs，将会乘上权重\n",
    "                        pos_per_region.append(pos[region_2] * positive_weight)\n",
    "                    # print(pos_per_region)\n",
    "                    if neg.__contains__(region_2):     # 其它区域的negative pairs，将会乘上权重\n",
    "                        for neg_cls_2 in neg[region_2]:\n",
    "                            neg_per_region.append(neg[region_2][neg_cls_2] * negative_weight)\n",
    "            if pos_per_region != [] and neg_per_region != []:    # 否则会报错，stack不能对empty list操作\n",
    "                pos_set_cl = torch.stack(pos_per_region, dim=0).cuda(non_blocking=True)       # 第一行tensor就是用于query的positive pair头，剩下都是所有乘上权重后的positive pairs\n",
    "                neg_set_cl = torch.stack(neg_per_region, dim=0).cuda(non_blocking=True)       # 所有乘上权重后的negative pairs\n",
    "                \n",
    "                # cl_per_region = contrastive_loss(pos_set_cl, neg_set_cl, temperature)\n",
    "                cl_per_region = contrastive_loss_criterion(pos_set_cl, neg_set_cl, temperature)\n",
    "                # print(cl_per_region)\n",
    "                loss.append(cl_per_region)\n",
    "\n",
    "            \n",
    "if loss != []:\n",
    "        loss = torch.stack(loss, dim=0).cuda(non_blocking=True)  \n",
    "        loss = torch.mean(loss)    \n",
    "else:\n",
    "    loss = torch.tensor([0]).cuda(non_blocking=True)\n",
    "print(loss)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862c12b9",
   "metadata": {},
   "source": [
    "### inter-images level contrastive loss 编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a4de433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2716, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# inter images contrastive loss v1\n",
    "from core.loss.contrastive_loss import ContrastiveLoss\n",
    "\n",
    "num_classes = 19\n",
    "positive_weight_increment_step = 0.01\n",
    "negative_weight_increment_step = 0.01\n",
    "temperature = 0.07\n",
    "\n",
    "loss = []\n",
    "\n",
    "contrastive_loss_criterion = ContrastiveLoss()\n",
    "\n",
    "for k in batch_centroids:\n",
    "    # calculate per image\n",
    "    for j in batch_centroids:\n",
    "        if k != j:\n",
    "            # calculate per class\n",
    "            for cls in range(num_classes):      # pos: 另一张图片的positive samples，pos_origin：该张图片的positive samples\n",
    "                pos_origin, pos, neg = {}, {}, {}\n",
    "                # pos: {'region_{}_{}': {tensor([...])} }\n",
    "                # neg: {'region_{}_{}': {cls: tensor([...]), cls: tensor([...]), cls: tensor([...]), ...}}\n",
    "                for region in batch_centroids[j]:  # 找到每一个region里该cls对应的其它所有pos所在的region和neg所在的region\n",
    "                    neg[region] = {}\n",
    "                    for inter_cls in batch_centroids[j][region]:\n",
    "                        neg[region][inter_cls] = batch_centroids[j][region][inter_cls]    # 只有这样copy tensor，才不至于改变batch_centroids本身\n",
    "                    if batch_centroids[j][region].__contains__(cls):\n",
    "                        pos[region] = batch_centroids[j][region][cls]     # 另一张图片的该区域的pos_region\n",
    "                        del neg[region][cls]\n",
    "                    if batch_centroids[k][region].__contains__(cls):        # 本张图片的该区域的pos_region\n",
    "                        pos_origin[region] = batch_centroids[k][region][cls]\n",
    "\n",
    "                pos_origin_region, pos_region, neg_region = [], [], []\n",
    "                pos_region = list(pos.keys()) # positive pairs所在的batch内另一张图片的region\n",
    "                neg_region = list(neg.keys()) # negative pairs所在的batch内另一张图片region\n",
    "                pos_origin_region = list(pos_origin.keys()) # positive pairs所在的batch内此张图片的region\n",
    "                inter_region = list(set(pos_region + neg_region))   # 另一张图片上pos和neg sample的所在region\n",
    "                all_region = list(set(pos_origin_region + pos_region + neg_region)) # 合并多个list，并且删除重复元素\n",
    "                \n",
    "                for region_1 in all_region:\n",
    "                    pos_per_region, neg_per_region = [], []         # 该类以该region为中心的positive和negative pairs\n",
    "                    cl_per_region = []\n",
    "                    region_1_index = np.array([int(region_1.split('_')[0]), int(region_1.split('_')[1])])\n",
    "                    \n",
    "                    if pos_origin.__contains__(region_1):\n",
    "                        pos_per_region.append(pos_origin[region_1])    # positive pairs的头，即cls在该图中region_1的centroids     \n",
    "                    \n",
    "                    for neg_cls_1 in neg[region_1]:\n",
    "                        neg_per_region.append(neg[region_1][neg_cls_1])   # negatiave pairs的头，即在region_1中除了cls之外的所有centroids\n",
    "\n",
    "                    for region_2 in inter_region:                         # 收集另一张图片每个region的positive pairs和negative pairs\n",
    "                        region_2_index = np.array([int(region_2.split('_')[0]), int(region_2.split('_')[1])])\n",
    "                        positive_weight = 1 + positive_weight_increment_step * np.linalg.norm(region_1_index - region_2_index, ord=2) # L2 norm\n",
    "                        negative_weight = 1 - negative_weight_increment_step * np.linalg.norm(region_1_index - region_2_index, ord=2) # L2 norm\n",
    "\n",
    "                        if pos.__contains__(region_2):     # 其它区域的positive pairs，将会乘上权重\n",
    "                            pos_per_region.append(pos[region_2] * positive_weight)\n",
    "\n",
    "                        if neg.__contains__(region_2):     # 其它区域的negative pairs，将会乘上权重\n",
    "                            for neg_cls_2 in neg[region_2]:\n",
    "                                neg_per_region.append(neg[region_2][neg_cls_2] * negative_weight)\n",
    "\n",
    "                    if pos_per_region != [] and neg_per_region != []:    # 否则会报错，stack不能对empty list操作\n",
    "                        pos_set_cl = torch.stack(pos_per_region, dim=0).cuda(non_blocking=True)       # 第一行tensor就是用于query的positive pair头，剩下都是所有乘上权重后的positive pairs\n",
    "                        neg_set_cl = torch.stack(neg_per_region, dim=0).cuda(non_blocking=True)       # 所有乘上权重后的negative pairs\n",
    "                        \n",
    "                        cl_per_region = contrastive_loss_criterion(pos_set_cl, neg_set_cl, temperature)\n",
    "\n",
    "                        loss.append(cl_per_region)\n",
    "            \n",
    "if loss != []:\n",
    "        loss = torch.stack(loss, dim=0).cuda(non_blocking=True)  \n",
    "        loss = torch.mean(loss)    \n",
    "else:\n",
    "    loss = torch.tensor([0]).cuda(non_blocking=True)\n",
    "print(loss)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e011d54",
   "metadata": {},
   "source": [
    "### cross-domain level contrastive loss 编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "107281a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.region_spliter import RegionSplit_CentroidCal\n",
    "batch_centroids_src = RegionSplit_CentroidCal(\n",
    "    predict=torch.softmax(src_out, dim=1),\n",
    "    label=src_label,\n",
    "    is_source=True,\n",
    "    numparts_h=2, \n",
    "    numparts_w=4\n",
    ")\n",
    "batch_centroids_tgt = RegionSplit_CentroidCal(\n",
    "    predict=torch.softmax(tgt_out, dim=1),\n",
    "    label=tgt_label,\n",
    "    is_source=False,\n",
    "    numparts_h=2, \n",
    "    numparts_w=4,\n",
    "    tgt_centroids_base_ratio=0.9,\n",
    "    tgt_out=tgt_out\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a4de433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "0 1\n",
      "1 0\n",
      "1 1\n",
      "tensor(4.1016, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# cross-domain images contrastive loss v1\n",
    "from core.loss.contrastive_loss import ContrastiveLoss\n",
    "\n",
    "num_classes = 19\n",
    "positive_weight_increment_step = 0.01\n",
    "negative_weight_increment_step = 0.01\n",
    "temperature = 0.07\n",
    "\n",
    "loss = []\n",
    "\n",
    "contrastive_loss_criterion = ContrastiveLoss()\n",
    "\n",
    "for cl_round in [0,1]:\n",
    "    if cl_round == 0:\n",
    "        batch_centroids_0 = batch_centroids_src\n",
    "        batch_centroids_1 = batch_centroids_tgt\n",
    "    else:\n",
    "        batch_centroids_1 = batch_centroids_src\n",
    "        batch_centroids_0 = batch_centroids_tgt\n",
    "    for k in batch_centroids_0:\n",
    "        # calculate per image from another domain\n",
    "        for j in batch_centroids_1:\n",
    "            # calculate per class\n",
    "            print(k,j)\n",
    "            for cls in range(num_classes):      # pos: 另一张图片的positive samples，pos_origin：该张图片的positive samples\n",
    "                pos_origin, pos, neg = {}, {}, {}\n",
    "                # pos: {'region_{}_{}': {tensor([...])} }\n",
    "                # neg: {'region_{}_{}': {cls: tensor([...]), cls: tensor([...]), cls: tensor([...]), ...}}\n",
    "                for region in batch_centroids_1[j]:  # 找到每一个region里该cls对应的其它所有pos所在的region和neg所在的region\n",
    "                    neg[region] = {}\n",
    "                    for inter_cls in batch_centroids_1[j][region]:\n",
    "                        neg[region][inter_cls] = batch_centroids_1[j][region][inter_cls]    # 只有这样copy tensor，才不至于改变batch_centroids本身\n",
    "                    if batch_centroids_1[j][region].__contains__(cls):\n",
    "                        pos[region] = batch_centroids_1[j][region][cls]     # 另一张图片的该区域的pos_region\n",
    "                        del neg[region][cls]\n",
    "                    if batch_centroids_0[k][region].__contains__(cls):        # 本张图片的该区域的pos_region\n",
    "                        pos_origin[region] = batch_centroids_0[k][region][cls]\n",
    "\n",
    "                pos_origin_region, pos_region, neg_region = [], [], []\n",
    "                pos_region = list(pos.keys()) # positive pairs所在的batch内另一张图片的region\n",
    "                neg_region = list(neg.keys()) # negative pairs所在的batch内另一张图片region\n",
    "                pos_origin_region = list(pos_origin.keys()) # positive pairs所在的batch内此张图片的region\n",
    "\n",
    "                inter_region = list(set(pos_region + neg_region))   # 另一张图片上pos和neg sample的所在region\n",
    "                all_region = list(set(pos_origin_region + pos_region + neg_region)) # 合并多个list，并且删除重复元素\n",
    "                \n",
    "                for region_1 in all_region:\n",
    "                    pos_per_region, neg_per_region = [], []         # 该类以该region为中心的positive和negative pairs\n",
    "                    cl_per_region = []\n",
    "                    region_1_index = np.array([int(region_1.split('_')[0]), int(region_1.split('_')[1])])\n",
    "                    \n",
    "                    if pos_origin.__contains__(region_1):\n",
    "                        pos_per_region.append(pos_origin[region_1])    # positive pairs的头，即cls在该图中region_1的centroids     \n",
    "                    \n",
    "                    for neg_cls_1 in neg[region_1]:\n",
    "                        neg_per_region.append(neg[region_1][neg_cls_1])   # negatiave pairs的头，即在region_1中除了cls之外的所有centroids\n",
    "\n",
    "                    for region_2 in inter_region:                         # 收集另一张图片每个region的positive pairs和negative pairs\n",
    "                        region_2_index = np.array([int(region_2.split('_')[0]), int(region_2.split('_')[1])])\n",
    "                        positive_weight = 1 + positive_weight_increment_step * np.linalg.norm(region_1_index - region_2_index, ord=2) # L2 norm\n",
    "                        negative_weight = 1 - negative_weight_increment_step * np.linalg.norm(region_1_index - region_2_index, ord=2) # L2 norm\n",
    "\n",
    "                        if pos.__contains__(region_2):     # 其它区域的positive pairs，将会乘上权重\n",
    "                            pos_per_region.append(pos[region_2] * positive_weight)\n",
    "\n",
    "                        if neg.__contains__(region_2):     # 其它区域的negative pairs，将会乘上权重\n",
    "                            for neg_cls_2 in neg[region_2]:\n",
    "                                neg_per_region.append(neg[region_2][neg_cls_2] * negative_weight)\n",
    "\n",
    "                    if pos_per_region != [] and neg_per_region != []:    # 否则会报错，stack不能对empty list操作\n",
    "                        pos_set_cl = torch.stack(pos_per_region, dim=0).cuda(non_blocking=True)       # 第一行tensor就是用于query的positive pair头，剩下都是所有乘上权重后的positive pairs\n",
    "                        neg_set_cl = torch.stack(neg_per_region, dim=0).cuda(non_blocking=True)       # 所有乘上权重后的negative pairs\n",
    "                        \n",
    "                        cl_per_region = contrastive_loss_criterion(pos_set_cl, neg_set_cl, temperature)\n",
    "                        \n",
    "                        loss.append(cl_per_region)\n",
    "                \n",
    "if loss != []:\n",
    "        loss = torch.stack(loss, dim=0).cuda(non_blocking=True)  \n",
    "        loss = torch.mean(loss)    \n",
    "else:\n",
    "    loss = torch.tensor([0]).cuda(non_blocking=True)\n",
    "print(loss)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998fd8c",
   "metadata": {},
   "source": [
    "### Early Stage Annotation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0673e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.region_spliter import RegionSplit_CentroidCal\n",
    "global_batch_centroids_src = RegionSplit_CentroidCal(\n",
    "    predict=torch.softmax(src_out, dim=1),\n",
    "    label=src_label,\n",
    "    is_source=True,\n",
    "    numparts_h=1, \n",
    "    numparts_w=1\n",
    ")\n",
    "batch_centroids_tgt = RegionSplit_CentroidCal(\n",
    "    predict=torch.softmax(tgt_out, dim=1),\n",
    "    label=tgt_label,\n",
    "    is_source=False,\n",
    "    numparts_h=2, \n",
    "    numparts_w=4,\n",
    "    tgt_centroids_base_ratio=0.9,\n",
    "    tgt_out=tgt_out\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2551020",
   "metadata": {},
   "source": [
    "#### Similarity Score Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "747f96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import unsqueeze\n",
    "import torch.nn.functional as F\n",
    "\n",
    "h = 640\n",
    "w = 1280\n",
    "numparts_h = 2  #此处应和上面生成的batch_centroids_tgt一致\n",
    "numparts_w = 4  #此处应和上面生成的batch_centroids_tgt一致\n",
    "parts_h = int(h / numparts_h)\n",
    "parts_w = int(w / numparts_w)\n",
    "\n",
    "tgt_predict = torch.softmax(tgt_out, dim=1)\n",
    "# print(tgt_predict.size())     # torch.Size([2, 19, 640, 1280])\n",
    "# print(tgt_label.size())       # torch.Size([2, 640, 1280])\n",
    "early_anno_score = torch.zeros(tgt_label.size()).cuda()\n",
    "early_anno_score[:,:,:] = 0\n",
    "\n",
    "for img_idx in batch_centroids_tgt:\n",
    "    for region_index in batch_centroids_tgt[img_idx]:\n",
    "        i, j = int(region_index.split('_')[0]), int(region_index.split('_')[1])\n",
    "        \n",
    "        if [i,j] == [range(numparts_h)[-1], range(numparts_w)[-1]]:\n",
    "            rg_id = [i*parts_h, h-1, j*parts_w, w-1]\n",
    "        else:\n",
    "            rg_id = [i*parts_h, (i+1)*parts_h-1, j*parts_w, (j+1)*parts_w-1]\n",
    "        for cls in batch_centroids_tgt[img_idx][region_index]:\n",
    "            if global_batch_centroids_src[img_idx]['0_0'].__contains__(cls) and global_batch_centroids_src[1-img_idx]['0_0'].__contains__(cls):\n",
    "                src_cls_prototype = (global_batch_centroids_src[img_idx]['0_0'][cls] + global_batch_centroids_src[1-img_idx]['0_0'][cls]) / len(global_batch_centroids_src)\n",
    "            elif global_batch_centroids_src[img_idx]['0_0'].__contains__(cls):\n",
    "                src_cls_prototype = global_batch_centroids_src[img_idx]['0_0'][cls]\n",
    "            elif global_batch_centroids_src[1-img_idx]['0_0'].__contains__(cls):\n",
    "                src_cls_prototype = global_batch_centroids_src[1-img_idx]['0_0'][cls]\n",
    "            else:\n",
    "                continue\n",
    "            tgt_cls_prototype = batch_centroids_tgt[img_idx][region_index][cls]\n",
    "            cross_cls_unsimilarity = torch.tensor([1]).cuda() - F.cosine_similarity(src_cls_prototype, tgt_cls_prototype, dim=0)\n",
    "            cls_mask = tgt_label[img_idx, rg_id[0]:rg_id[1], rg_id[2]:rg_id[3]].eq(cls)\n",
    "            intra_cls_similarity = F.cosine_similarity(tgt_predict[img_idx, :, rg_id[0]:rg_id[1], rg_id[2]:rg_id[3]], tgt_cls_prototype.unsqueeze(dim=1).unsqueeze(dim=2), dim=0)\n",
    "            score_mask = cls_mask * cross_cls_unsimilarity * intra_cls_similarity\n",
    "            early_anno_score[img_idx, rg_id[0]:rg_id[1], rg_id[2]:rg_id[3]] += score_mask\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# print(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd328f",
   "metadata": {},
   "source": [
    "#### Uncertainty Score Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3820063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.active.early_anno_score import EarlyAnnoScore\n",
    "src_predict = torch.softmax(src_out, dim=1)\n",
    "batch_centroids_tgt=RegionSplit_CentroidCal(predict=tgt_predict, \n",
    "                                                        label=tgt_label, \n",
    "                                                        is_source=False,\n",
    "                                                        numparts_h=2, \n",
    "                                                        numparts_w=4,\n",
    "                                                        tgt_centroids_base_ratio=0.9,\n",
    "                                                        tgt_out=tgt_out)\n",
    "global_batch_centroids_src=RegionSplit_CentroidCal(predict=src_predict, \n",
    "                                                        label=src_label, \n",
    "                                                        is_source=True,\n",
    "                                                        numparts_h=1, \n",
    "                                                        numparts_w=1)\n",
    "score = EarlyAnnoScore(batch_centroids_tgt=batch_centroids_tgt,\n",
    "                        global_batch_centroids_src=global_batch_centroids_src,\n",
    "                        tgt_label=tgt_label,\n",
    "                        tgt_predict=tgt_predict,\n",
    "                        numparts_h=2,\n",
    "                        numparts_w=4\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81cecacf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'origin_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bperlmutter/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m origin_size\n",
      "\u001b[0;31mNameError\u001b[0m: name 'origin_size' is not defined"
     ]
    }
   ],
   "source": [
    "origin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1867bf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 640, 1280])\n",
      "torch.Size([2, 640, 1280])\n"
     ]
    }
   ],
   "source": [
    "print(tgt_label.unsqueeze(dim=0).size())\n",
    "print(tgt_label.float().size())\n",
    "tgt_label_interpolation = F.interpolate(tgt_label.float().unsqueeze(dim=0), size=(1024, 2048), mode='bilinear', align_corners=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9db33bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 1280]) torch.Size([640, 1280])\n"
     ]
    }
   ],
   "source": [
    "values, indices = torch.max(score, dim=0)\n",
    "print(values.size(), indices.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e66f37a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:08,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:09,  4.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bperlmutter/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mfor\u001b[39;00m pixel \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(active_budget):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bperlmutter/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     \u001b[39mif\u001b[39;00m pixel \u001b[39m<\u001b[39m similarity_budget:      \u001b[39m# similarity annotation\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bperlmutter/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m         values, indices_h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmax(similarity_score, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bperlmutter/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39melse\u001b[39;00m:                                        \u001b[39m# uncertainty annotation\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bperlmutter/global/cfs/cdirs/m1759/Chen_Zhichao/RCL_AAA/train.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m         values, indices_h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(uncertainty_score, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from core.active.floating_region import FloatingRegionScore\n",
    "from core.active.spatial_purity import SpatialPurity\n",
    "from core.active.early_anno_score import EarlyAnnoScore\n",
    "\n",
    "now_iteration=10000\n",
    "max_iter = 50000\n",
    "\n",
    "feature_extractor.eval()\n",
    "classifier.eval()\n",
    "\n",
    "\n",
    "# floating_region_score = FloatingRegionScore(in_channels=cfg.MODEL.NUM_CLASSES, size=2 * cfg.ACTIVE.RADIUS_K + 1).cuda()\n",
    "# per_region_pixels = (2 * cfg.ACTIVE.RADIUS_K + 1) ** 2\n",
    "# active_radius = cfg.ACTIVE.RADIUS_K\n",
    "# mask_radius = cfg.ACTIVE.RADIUS_K * 2\n",
    "active_ratio = cfg.ACTIVE.RATIO / len(cfg.ACTIVE.SELECT_ITER)\n",
    "\n",
    "flag = 1\n",
    "cross_entropy_computation = nn.CrossEntropyLoss(ignore_index=255, reduction='none')\n",
    "with torch.no_grad():\n",
    "    for (src_data, tgt_data) in tqdm(zip(src_train_loader, tgt_epoch_loader)):\n",
    "\n",
    "        src_input, src_label = src_data['img'], src_data['label']\n",
    "        src_input = src_input.cuda(non_blocking=True)\n",
    "        src_label = src_label.cuda(non_blocking=True)\n",
    "\n",
    "        src_size = src_input.shape[-2:]\n",
    "        src_out = classifier(feature_extractor(src_input), size=src_size)\n",
    "        src_predict = torch.softmax(src_out, dim=1)\n",
    "\n",
    "        tgt_input, path2mask = tgt_data['img'], tgt_data['path_to_mask']    # tgt_input: torch.Size([1, 3, 640, 1280])\n",
    "        origin_mask, origin_label = tgt_data['origin_mask'], tgt_data['origin_label']       # origin_mask, origin_label: torch.Size([1, 1024, 2048])\n",
    "        origin_size = tgt_data['size']\n",
    "        active_indicator = tgt_data['active']\n",
    "        selected_indicator = tgt_data['selected']\n",
    "        path2indicator = tgt_data['path_to_indicator']\n",
    "\n",
    "        tgt_input = tgt_input.cuda(non_blocking=True)\n",
    "\n",
    "        tgt_size = tgt_input.shape[-2:]\n",
    "        tgt_feat = feature_extractor(tgt_input)\n",
    "        tgt_out = classifier(tgt_feat, size=tgt_size)       # tgt_out: torch.Size([1, 19, 640, 1280])\n",
    "        tgt_predict = torch.softmax(tgt_out, dim=1)\n",
    "        tgt_label = torch.argmax(tgt_predict[:,:,:,:],dim=1)    # tgt_label: torch.Size([1, 640, 1280])\n",
    "\n",
    "        for i in range(len(origin_mask)):       # 一个batch内的图片数量  # origin_mask, origin_label: torch.Size([1, 1024, 2048])\n",
    "            active_mask = origin_mask[i].cuda(non_blocking=True)\n",
    "            ground_truth = origin_label[i].cuda(non_blocking=True)\n",
    "            size = (origin_size[i][0], origin_size[i][1])       # size: tensor(1024), tensor(2048)\n",
    "            num_pixel_cur = size[0] * size[1]\n",
    "            active = active_indicator[i]        # torch.Size([1024, 2048])，最开始都是False\n",
    "            selected = selected_indicator[i]\n",
    "\n",
    "            output = tgt_out[i:i + 1, :, :, :]\n",
    "            output = F.interpolate(output, size=size, mode='bilinear', align_corners=True)\n",
    "            # score, purity, entropy = floating_region_score(output, now_iteration=now_iteration, cfg=cfg)\n",
    "\n",
    "            tgt_label_interpolation = F.interpolate(tgt_label.float().unsqueeze(dim=0), size=size, mode='bilinear', align_corners=True)\n",
    "            tgt_label_interpolation = tgt_label_interpolation.squeeze(dim=0).long()     # torch.Size([1, 1024, 2048]) \n",
    "            tgt_predict_interpolation = F.interpolate(tgt_predict, size=size, mode='bilinear', align_corners=True)  # torch.Size([1, 19, 1024, 2048])\n",
    "\n",
    "            batch_centroids_tgt=RegionSplit_CentroidCal(predict=tgt_predict_interpolation, \n",
    "                                                        label=tgt_label_interpolation, \n",
    "                                                        is_source=False,\n",
    "                                                        numparts_h=2, \n",
    "                                                        numparts_w=4,\n",
    "                                                        tgt_centroids_base_ratio=0.9,\n",
    "                                                        tgt_out=output) # !!!\n",
    "            global_batch_centroids_src=RegionSplit_CentroidCal(predict=src_predict, \n",
    "                                                                    label=src_label, \n",
    "                                                                    is_source=True,\n",
    "                                                                    numparts_h=1, \n",
    "                                                                    numparts_w=1)\n",
    "            \n",
    "       \n",
    "            similarity_score = EarlyAnnoScore(batch_centroids_tgt=batch_centroids_tgt,\n",
    "                                    global_batch_centroids_src=global_batch_centroids_src,\n",
    "                                    tgt_label=tgt_label_interpolation,\n",
    "                                    tgt_predict=tgt_predict_interpolation,\n",
    "                                    numparts_h=2,\n",
    "                                    numparts_w=4)\n",
    "\n",
    "            # similarity_score: torch.Size([1, 1024, 2048])\n",
    "            similarity_score = similarity_score.squeeze(dim=0) # similarity_score: torch.Size([1024, 2048])\n",
    "            similarity_score[active] = 0.0     # 把上一轮已经标注过的pixel给置0\n",
    "\n",
    "            # uncertainty_score: torch.Size([1, 1024, 2048])\n",
    "            uncertainty_score = cross_entropy_computation(output, tgt_label_interpolation)\n",
    "            uncertainty_score = uncertainty_score.squeeze(dim=0)\n",
    "\n",
    "            active_budget = math.ceil(num_pixel_cur * active_ratio)    # 将要actively selected pixel的数量\n",
    "            similarity_budget = math.ceil(active_budget * ((now_iteration - max_iter) / max_iter) ** 2)\n",
    "            uncertainty_budget = active_budget - similarity_budget\n",
    "\n",
    "            for pixel in range(active_budget):\n",
    "                if pixel < similarity_budget:      # similarity annotation\n",
    "                    values, indices_h = torch.max(similarity_score, dim=0)\n",
    "                else:                                        # uncertainty annotation\n",
    "                    values, indices_h = torch.max(uncertainty_score, dim=0)\n",
    "                _, indices_w = torch.max(values, dim=0)\n",
    "                w = indices_w.item()\n",
    "                h = indices_h[w].item()\n",
    "\n",
    "                # mask out\n",
    "                similarity_score[h,w] = 0.0\n",
    "                uncertainty_score[h,w] = 0.0\n",
    "                active[h,w] = True\n",
    "                selected[h,w] = True\n",
    "                # active sampling\n",
    "                active_mask[h,w] = ground_truth[h,w]\n",
    "                \n",
    "\n",
    "            active_mask = Image.fromarray(np.array(active_mask.cpu().numpy(), dtype=np.uint8))\n",
    "            active_mask.save(path2mask[i])\n",
    "            indicator = {\n",
    "                'active': active,\n",
    "                'selected': selected\n",
    "            }\n",
    "            torch.save(indicator, path2indicator[i])\n",
    "\n",
    "feature_extractor.train()\n",
    "classifier.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('MY_PROJ')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "02f5b6f99c7d7520503bf7e72d414878432f2f5acd2aefdb81eb5ace38f206e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
